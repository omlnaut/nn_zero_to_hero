{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchify batch norm notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from [here](https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4715s)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Layer\n",
    "  - n_inputs, n_outputs, bias-flag\n",
    "  - pass is done in call method\n",
    "  - parameters returns list\n",
    "  - remember to scale weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out, bias=True) -> None:\n",
    "        self.W = torch.randn((n_in, n_out), requires_grad=True)\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.b = torch.zeros((1,n_out), requires_grad=True)\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.W\n",
    "        if self.use_bias:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.use_bias: return [self.W, self.b]\n",
    "        return [self.W]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNorm1d\n",
    "  - dim\n",
    "  - eps (division by std)\n",
    "  - momentum\n",
    "  - training-flag to toggle running-mean calculation\n",
    "  - keep track of var instead of std\n",
    "  - in training:\n",
    "    - use exact mean/std of batch, still update running statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, train=True, momentum=.1, eps=1e-5):\n",
    "        self.dim = dim\n",
    "        self.train = train\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.running_mean = torch.zeros((1,dim), requires_grad=False)\n",
    "        self.running_var = torch.ones((1,dim), requires_grad=False)\n",
    "\n",
    "        self.gain = torch.zeros((1,dim), requires_grad=True)\n",
    "        self.scale = torch.ones((1,dim), requires_grad=True)\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        if self.train:\n",
    "            batch_mean = x.mean(dim=0, keepdim=True)\n",
    "            batch_var = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            batch_mean = self.running_mean\n",
    "            batch_var = self.running_var\n",
    "\n",
    "        self.out = (x - batch_mean) / batch_var\n",
    "        self.out = self.out * self.scale + self.gain\n",
    "\n",
    "        if self.train:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum)*self.running_mean + self.momentum*batch_mean\n",
    "                self.running_var = (1-self.momentum)*self.running_var + self.momentum*batch_var\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gain, self.scale]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tanh\n",
    "  - simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, emb_size, n_in):\n",
    "        self.emb_size = emb_size\n",
    "        self.n_in = n_in\n",
    "        self.C = torch.randn((n_in, emb_size), requires_grad=True)\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        self.out = self.C[x] # embed the characters into vectors\n",
    "        self.out = self.out.view(self.out.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self): return [self.C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general:\n",
    "  - use self.out to store output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    data = Path('../data/')\n",
    "    return torch.load(data/filename)\n",
    "\n",
    "\n",
    "x_train = load('x_train')\n",
    "y_train = load('y_train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build 6 layer network\n",
    "  - scale last layer weights down by .1\n",
    "  - scale all other layers by 5/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layer_list):\n",
    "        self.layer_list = layer_list\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return sum((layer.parameters() for layer in self.layer_list), start=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "emb_size = 10\n",
    "n_hidden = 100\n",
    "vocab_size = 27\n",
    "\n",
    "net = Sequential([\n",
    "    Embedding(emb_size, vocab_size),\n",
    "    Linear(3*emb_size, n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "])\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "net.layer_list[-1].W.data *= .1\n",
    "for layer in net.layer_list[:1]:\n",
    "    if isinstance(layer, Linear):\n",
    "        layer.W.data *= 5/3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training loop\n",
    "  - embed\n",
    "  - linear application of all layers\n",
    "  - cross entropy\n",
    "  - retain_graph on all layer.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/      1: 3.8727\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, x_train.shape[0], (batch_size,), generator=g)\n",
    "    xb, yb = x_train[ix], y_train[ix] # batch X,Y\n",
    "\n",
    "    x = net(xb)\n",
    "    \n",
    "    loss = F.cross_entropy(x, yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for layer in net.layer_list[1:]:\n",
    "        layer.out.retain_grad()\n",
    "\n",
    "    for p in net.parameters():\n",
    "      p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in net.parameters():\n",
    "      p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualization\n",
    "  - for each layer, take out of tanh\n",
    "    - mean, std, percent of >.97\n",
    "  - plot histograms for each layer into one diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same visualization for grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e98ef4f9346bee673f51c6ffc649e77c3dd738815ba08794b113970bacedb832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
