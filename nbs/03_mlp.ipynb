{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following https://www.youtube.com/watch?v=TCH_1BHY58I&t=188s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuilding Dataset ( -  00:12:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load dataset\n",
    "- build character-index lookup table\n",
    "- build (xxx) -> y dataset (as tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = Path('../data/names.txt').open().read().splitlines()\n",
    "names[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(''.join(names))))\n",
    "characters.insert(0, '.')\n",
    "\n",
    "stoi = {c: i for i,c in enumerate(characters)}\n",
    "itos = {i: c for c,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "def sliding_window(iterable, size):\n",
    "    iters = tee(iterable, size)\n",
    "    for i, it in enumerate(iters):\n",
    "        for _ in range(i):\n",
    "            next(it, None)\n",
    "    return zip(*iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2)\n",
      "(1, 2, 3)\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "numbers = list(range(5))\n",
    "for window in sliding_window(numbers, 3):\n",
    "    print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', '.', '.') ---> e\n",
      "('.', '.', 'e') ---> m\n",
      "('.', 'e', 'm') ---> m\n",
      "('e', 'm', 'm') ---> a\n",
      "('m', 'm', 'a') ---> .\n",
      "('.', '.', '.') ---> o\n",
      "('.', '.', 'o') ---> l\n",
      "('.', 'o', 'l') ---> i\n",
      "('o', 'l', 'i') ---> v\n",
      "('l', 'i', 'v') ---> i\n",
      "('i', 'v', 'i') ---> a\n",
      "('v', 'i', 'a') ---> .\n",
      "('.', '.', '.') ---> a\n",
      "('.', '.', 'a') ---> v\n",
      "('.', 'a', 'v') ---> a\n",
      "('a', 'v', 'a') ---> .\n",
      "('.', '.', '.') ---> i\n",
      "('.', '.', 'i') ---> s\n",
      "('.', 'i', 's') ---> a\n",
      "('i', 's', 'a') ---> b\n",
      "('s', 'a', 'b') ---> e\n",
      "('a', 'b', 'e') ---> l\n",
      "('b', 'e', 'l') ---> l\n",
      "('e', 'l', 'l') ---> a\n",
      "('l', 'l', 'a') ---> .\n",
      "('.', '.', '.') ---> s\n",
      "('.', '.', 's') ---> o\n",
      "('.', 's', 'o') ---> p\n",
      "('s', 'o', 'p') ---> h\n",
      "('o', 'p', 'h') ---> i\n",
      "('p', 'h', 'i') ---> a\n",
      "('h', 'i', 'a') ---> .\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "context_size = 3\n",
    "\n",
    "for name in names[:5]:\n",
    "    name = context_size*['.'] + list(name) + ['.']\n",
    "    for chars in sliding_window(name, context_size+1):\n",
    "        print(f'{chars[:context_size]} ---> {chars[-1]}')\n",
    "        indices = list(map(lambda c: stoi[c], chars))\n",
    "        xs.append((indices[:context_size]))\n",
    "        ys.append(indices[-1])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Matrix ( - 00:18:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Embedding Matrix with embedding size 2 (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2), requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer ( - 00:29:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hidden layer of dimension (6, 100)\n",
    "    - weights and biases\n",
    "- Transform input from (batch_size, 3, embedding_size) to (batch_size, 3*embedding_size) to enable multiplication\n",
    "- activation function: tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = C[xs]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((6,100), requires_grad=True)\n",
    "b = torch.randn((1, 100), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = X.shape[0]\n",
    "reshaped = X.view(batch_size, -1)\n",
    "reshaped.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((reshaped @ W) + b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = torch.tanh((reshaped @ W) + b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output layer ( - 00:33:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from hidden_size(100) to 27 (amount of characters)\n",
    "- neg. likelihood like last notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (activation @ W2) + b2\n",
    "output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation of [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss):\n",
    "\n",
    "- input has to be a Tensor of size $(C)$ for unbatched input, $(minibatch, C)$ for batched input\n",
    "- The target should be class indices in the range $[0, C)$ where $C$ is the number of classes\n",
    "- Reduction is by default 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 27])\n",
      "Target shape:  torch.Size([32])\n",
      "Target values:  tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
      "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "print('Input shape: ', output.shape)\n",
    "print('Target shape: ', ys.shape)\n",
    "print('Target values: ', ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.7773, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(output, ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.7773, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "counts = output.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "# neg log likelihood\n",
    "logs = - probs.log()\n",
    "loss = logs[torch.arange(len(ys)), ys]\n",
    "loss.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring ( - 00:38:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define all matrices in one place\n",
    "- collect parameters in list\n",
    "    - count parameters, should be 3481\n",
    "- Use seed generator\n",
    "- define forward pass with F.cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "embedding_size = 2\n",
    "hidden_layer_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for name in names[:5]:\n",
    "    name = context_size*['.'] + list(name) + ['.']\n",
    "    for chars in sliding_window(name, context_size+1):\n",
    "        indices = list(map(lambda c: stoi[c], chars))\n",
    "        xs.append((indices[:context_size]))\n",
    "        ys.append(indices[-1])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  3481\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((27, embedding_size), requires_grad=True, generator=g)\n",
    "W1 = torch.randn((embedding_size*context_size, hidden_layer_size), requires_grad=True, generator=g)\n",
    "b1 = torch.randn(hidden_layer_size, requires_grad=True, generator=g)\n",
    "W2 = torch.randn((hidden_layer_size, 27), requires_grad=True, generator=g)\n",
    "b2 = torch.randn(27, requires_grad=True, generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print('Number of parameters: ', sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.769710540771484"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[xs].view(-1, embedding_size*context_size)\n",
    "\n",
    "inner = torch.tanh((embeddings @ W1) + b1)\n",
    "logits = (inner @ W2) + b2\n",
    "\n",
    "loss = F.cross_entropy(logits, ys)\n",
    "loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop ( - )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward pass\n",
    "- backward pass\n",
    "    - zero grads\n",
    "    - update by learning rate\n",
    "- repeat n times for the one batch (intentional overfitting)\n",
    "    - loss should go to ~.25 (1000 iterations)\n",
    "- comnpare logits max against ys (manually)\n",
    "    - should be close to actual indices\n",
    "\n",
    "- Train on full dataset\n",
    "    - slow\n",
    "- instead use minibatches of size 32\n",
    "    - randomly drawn in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e98ef4f9346bee673f51c6ffc649e77c3dd738815ba08794b113970bacedb832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
