{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(list(set(text)))\n",
    "''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch: i for i, ch in enumerate(tokens) }\n",
    "itos = { i: ch for i, ch in enumerate(tokens) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    return torch.tensor([stoi[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return ''.join([itos[i.item()] for i in tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58, 43, 57, 58, 47])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('testi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testi'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('testi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = encode(text)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([892315]) torch.Size([223079])\n"
     ]
    }
   ],
   "source": [
    "split = int(0.8 * len(data))\n",
    "train = data[:split]\n",
    "val = data[split:]\n",
    "\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a single chunk of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([64, 43, 52, 10,  0, 14, 43, 44])\n",
      "tensor([43, 52, 10,  0, 14, 43, 44, 53])\n"
     ]
    }
   ],
   "source": [
    "offset = 10 # arbitrary offset for demonstration\n",
    "\n",
    "x = train[offset:offset+block_size]\n",
    "y = train[offset+1:offset+block_size+1]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate random offsets into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([225210, 561086, 254928, 458402])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets = torch.randint(0, split-block_size, (batch_size,))\n",
    "offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then generate a block-size x and a shifted-by-1 block-size y for each offset, stacking those tensor into a single x and y tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46, 47, 51,  1, 44, 56, 53, 51],\n",
      "        [47, 52, 49,  1, 47, 58,  1, 53],\n",
      "        [42, 43, 44, 39, 41, 43, 42,  1],\n",
      "        [37,  1, 15, 13, 28, 33, 24, 17]])\n",
      "tensor([[47, 51,  1, 44, 56, 53, 51,  1],\n",
      "        [52, 49,  1, 47, 58,  1, 53, 44],\n",
      "        [43, 44, 39, 41, 43, 42,  1, 61],\n",
      "        [ 1, 15, 13, 28, 33, 24, 17, 32]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([data[offset : offset+block_size] for offset in offsets]))\n",
    "print(torch.stack([data[offset+1 : offset+block_size+1] for offset in offsets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, block_size=block_size, batch_size=batch_size):\n",
    "    offsets = torch.randint(0, split-block_size, (batch_size,))\n",
    "\n",
    "    xb = torch.stack([data[offset : offset+block_size] for offset in offsets])\n",
    "    yb = torch.stack([data[offset+1 : offset+block_size+1] for offset in offsets])\n",
    "\n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[47, 45, 46, 58,  6,  1, 39, 50],\n",
       "         [58,  1, 57, 53,  1, 58, 43, 52],\n",
       "         [ 1, 58, 46, 39, 58,  1, 63, 53],\n",
       "         [43, 43, 57,  1, 52, 53, 56,  1]]),\n",
       " tensor([[45, 46, 58,  6,  1, 39, 50, 50],\n",
       "         [ 1, 57, 53,  1, 58, 43, 52, 42],\n",
       "         [58, 46, 39, 58,  1, 63, 53, 59],\n",
       "         [43, 57,  1, 52, 53, 56,  1, 46]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return x, None\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(x.view(-1, self.vocab_size), targets.view(-1))\n",
    "\n",
    "        return x, loss\n",
    "\n",
    "    def generate_text(self, x, steps=500):\n",
    "        for _ in range(steps):\n",
    "            logits, _ = self(x)\n",
    "            last_logits = logits[:,-1,:]\n",
    "            probs = torch.functional.F.softmax(last_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, steps=100):\n",
    "        for _ in range(steps):\n",
    "            x, _ = self(x)\n",
    "            x = x[-1].argmax()\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokens)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch(train)\n",
    "xb.shape # (batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, loss = model(xb, yb)\n",
    "logits.shape # (batch_size, block_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7298, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 38, 32, 55, 34, 42,  3, 15, 11, 32,  6,  4, 49, 54, 11, 47, 61, 46,\n",
       "         28, 53,  3, 26, 60, 20, 13, 57,  8, 19, 48, 59,  7, 53, 58,  6, 28, 56,\n",
       "         44, 13, 48, 20, 33, 27, 30, 17, 56, 10, 35, 61, 10, 64,  5, 15, 36, 47,\n",
       "         41, 49, 57, 16,  8,  3, 23, 15,  6, 57, 18,  3, 18, 16, 14, 37, 41, 58,\n",
       "         48, 20, 55, 15, 20, 56,  7, 53,  3,  8, 58, 20, 13,  9, 55, 64,  4, 49,\n",
       "         39, 45,  2, 31, 21, 55, 24, 30,  3, 10,  2, 58, 20, 21, 31,  1,  6, 58,\n",
       "         34, 32,  3, 11, 18, 47, 38, 60, 56, 28,  8, 13, 46, 16, 37, 32, 16, 17,\n",
       "          0, 34, 42, 43, 25, 43, 12, 46, 42, 30, 51, 54, 45, 52,  3, 26, 22, 20,\n",
       "          5, 64, 13,  8, 12, 31, 32, 42,  1,  0, 60, 48, 56, 50,  7, 15, 17,  5,\n",
       "         41, 43,  4, 18, 36,  6, 20, 63,  5, 26, 27, 30, 61, 27, 43, 41, 58, 29,\n",
       "         33, 59,  2, 31, 32, 64, 21,  2, 34, 51, 54,  2, 19, 12, 31, 15, 59, 53,\n",
       "         15,  2,  1, 14, 30,  1, 24, 49, 18, 51, 10, 14, 11,  4, 17, 53, 50, 36,\n",
       "         34, 13,  4, 12, 31, 29, 52, 30, 39, 21, 33,  2, 17, 11,  2,  4, 17,  7,\n",
       "          1,  1,  4, 48, 35, 46, 14, 12, 31, 32, 38, 35, 18, 36, 47, 47, 54, 57,\n",
       "         57, 34, 58, 26, 21, 30, 57, 48, 15, 20, 45, 59,  6, 56, 35,  5, 25, 61,\n",
       "         20, 53, 52, 43, 64, 53, 52,  3, 53, 61, 29, 31, 45, 45, 35, 12, 31, 40,\n",
       "         42, 47, 32, 15, 62, 61, 17,  0, 19, 37, 51, 12, 16, 53, 18, 39, 61,  9,\n",
       "         25, 45,  1, 55, 43, 12, 47, 59,  1,  7, 63, 24, 15, 49, 37,  8, 28, 30,\n",
       "         31, 32, 48, 20, 15,  1, 33, 58, 25, 41, 28, 37, 37, 64, 10,  0, 51, 17,\n",
       "         12, 16, 37, 51, 52,  8, 14, 25, 26,  0, 61, 10, 31, 32, 63,  5, 22, 28,\n",
       "         20,  0, 22, 44, 33, 39, 22, 14,  0,  9, 37, 11, 61, 56, 35, 42, 63, 17,\n",
       "         12, 17,  4, 17, 49, 43, 56, 29,  4, 49, 49, 52, 38, 32, 34, 38, 25,  5,\n",
       "         17, 63, 54,  4, 58,  6, 62, 31, 34, 44,  9, 24, 58, 10, 41, 62,  7, 48,\n",
       "          1, 64, 62, 61, 16,  3, 21, 51,  3, 32, 37, 27, 41, 33, 15, 57, 10, 64,\n",
       "         28, 43, 35, 17, 25, 44, 56, 34, 50, 37, 53, 42,  5, 25, 38, 12, 41, 15,\n",
       "         36, 17, 12, 51, 46,  8, 13, 29, 40, 42, 14,  0,  0, 31, 56, 43, 56, 47,\n",
       "         41, 13, 46, 26, 18, 31, 15, 15, 17, 53,  3, 25, 27,  5, 15, 17, 28, 11,\n",
       "         13, 15, 38, 32, 14, 12,  7, 36, 14, 37, 61, 27, 35, 28, 10]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1,1), dtype=torch.long) # BxT\n",
    "x = model.generate_text(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZTqVd$C;T,&kp;iwhPo$NvHAs.Gju-ot,PrfAjHUOREr:Ww:z'CXicksD.$KC,sF$FDBYctjHqCHr-o$.tHA3qz&kag!SIqLR$:!tHIS ,tVT$;FiZvrP.AhDYTDE\n",
      "VdeMe?hdRmpgn$NJH'zA.?STd \n",
      "vjrl-CE'ce&FX,Hy'NORwOectQUu!STzI!Vmp!G?SCuoC! BR LkFm:B;&EolXVA&?SQnRaIU!E;!&E-  &jWhB?STZWFXiipssVtNIRsjCHgu,rW'MwHonezon$owQSggW?SbdiTCxwE\n",
      "GYm?DoFaw3Mg qe?iu -yLCkY.PRSTjHC UtMcPYYz:\n",
      "mE?DYmn.BMN\n",
      "w:STy'JPH\n",
      "JfUaJB\n",
      "3Y;wrWdyE?E&EkerQ&kknZTVZM'Eyp&t,xSVf3Lt:cx-j zxwD$Im$TYOcUCs:zPeWEMfrVlYod'MZ?cCXE?mh.AQbdB\n",
      "\n",
      "SrericAhNFSCCEo$MO'CEP;ACZTB?-XBYwOWP:\n"
     ]
    }
   ],
   "source": [
    "print(decode(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
