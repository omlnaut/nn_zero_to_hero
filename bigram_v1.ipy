import torch

train_split = 0.8
block_size = 8
batch_size = 32


def load_data() -> str:
    with open("data/shakespeare.txt", "r") as f:
        text = f.read()

    return text


class Tokenizer:
    def __init__(self, text: str):
        self.text = text
        self.tokens = sorted(list(set(text)))

        self.stoi = {ch: i for i, ch in enumerate(self.tokens)}
        self.itos = {i: ch for i, ch in enumerate(self.tokens)}

    def encode(self, text: str) -> torch.Tensor:
        return torch.tensor([self.stoi[ch] for ch in text], dtype=torch.long)

    def decode(self, tensor: torch.Tensor) -> str:
        return "".join([self.itos[int(i.item())] for i in tensor])

    @property
    def vocab_size(self):
        return len(self.tokens)


def get_batch(data, block_size=block_size, batch_size=batch_size):
    offsets = torch.randint(0, split - block_size, (batch_size,))

    xb = torch.stack([data[offset : offset + block_size] for offset in offsets])
    yb = torch.stack([data[offset + 1 : offset + block_size + 1] for offset in offsets])

    return xb, yb


class BigramModel(torch.nn.Module):
    def __init__(self, vocab_size):
        super().__init__()

        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)
        self.vocab_size = vocab_size

    def forward(self, x, targets=None):
        x = self.embedding(x)

        if targets is None:
            return x, None

        loss = torch.nn.functional.cross_entropy(
            x.view(-1, self.vocab_size), targets.view(-1)
        )

        return x, loss

    def generate_text(self, x, steps=500):
        for _ in range(steps):
            logits, _ = self(x)
            last_logits = logits[:, -1, :]
            probs = torch.functional.F.softmax(last_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            x = torch.cat([x, next_token], dim=1)

        return x

    @torch.no_grad()
    def generate(self, x, steps=100):
        for _ in range(steps):
            x, _ = self(x)
            x = x[-1].argmax()
            yield x


def train_loop(
    n_batches: int, model: BigramModel, data: torch.Tensor, lr: float = 0.01
):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    print_every = int(n_batches * 0.1)

    for epoch in range(n_batches):
        xb, yb = get_batch(data)

        optimizer.zero_grad()
        _, loss = model(xb, yb)

        if epoch % print_every == 0:
            print(f"Epoch {epoch}, loss: {loss.item()}")

        loss.backward()
        optimizer.step()


if __name__ == "__main__":
    text = load_data()
    tokenizer = Tokenizer(text)
    vocab_size = tokenizer.vocab_size

    data = tokenizer.encode(text)
    print(f"Length of data: {len(data)}")

    split = int(len(data) * train_split)
    train = data[:split]
    test = data[split:]
    print(f"Train size: {len(train)}")
    print(f"Test size: {len(test)}")

    model = BigramModel(vocab_size)

    train_loop(1000, model, train)

    print("Generating text...")
    text = model.generate_text(torch.zeros((1, 1), dtype=torch.long))
    decoded = tokenizer.decode(text[0])

    print(decoded)
