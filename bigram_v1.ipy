import torch

from transformer import SelfAttentionHead, Tokenizer

train_split = 0.8
block_size = 8
batch_size = 32
n_embed = 32


def load_data() -> str:
    with open("data/shakespeare.txt", "r") as f:
        text = f.read()

    return text


def get_batch(data, block_size=block_size, batch_size=batch_size):
    offsets = torch.randint(0, split - block_size, (batch_size,))

    xb = torch.stack([data[offset : offset + block_size] for offset in offsets])
    yb = torch.stack([data[offset + 1 : offset + block_size + 1] for offset in offsets])

    return xb, yb


class BigramModel(torch.nn.Module):
    def __init__(self, vocab_size):
        super().__init__()

        self.vocab_size = vocab_size

        self.embedding = torch.nn.Embedding(vocab_size, n_embed)
        self.pos_embedding = torch.nn.Embedding(block_size, n_embed)

        self.sa_head = SelfAttentionHead(n_embed, n_embed, n_embed)

        self.lm_head = torch.nn.Linear(n_embed, vocab_size)

    def forward(self, x, targets=None):
        B, T = x.shape

        token_embeddings = self.embedding(x)
        pos_embeddings = self.pos_embedding(torch.arange(T))

        token_embeddings += pos_embeddings

        token_embeddings = self.sa_head(token_embeddings)

        logits = self.lm_head(token_embeddings)

        if targets is None:
            return logits, None

        loss = torch.nn.functional.cross_entropy(
            logits.view(-1, self.vocab_size), targets.view(-1)
        )

        return logits, loss

    def generate_text(self, x, steps=500):
        for _ in range(steps):
            truncated_x = x[:, -block_size:]
            logits, _ = self(truncated_x)
            last_logits = logits[:, -1, :]  # B x T x C
            probs = torch.functional.F.softmax(last_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            x = torch.cat([x, next_token], dim=1)

        return x

    @torch.no_grad()
    def generate(self, x, steps=100):
        for _ in range(steps):
            x, _ = self(x)
            x = x[-1].argmax()
            yield x


def train_loop(
    n_batches: int, model: BigramModel, data: torch.Tensor, lr: float = 0.01
):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    print_every = int(n_batches * 0.1)

    for epoch in range(n_batches):
        xb, yb = get_batch(data)

        optimizer.zero_grad()
        _, loss = model(xb, yb)

        if epoch % print_every == 0:
            print(f"Epoch {epoch}, loss: {loss.item()}")

        loss.backward()
        optimizer.step()


if __name__ == "__main__":
    text = load_data()
    tokenizer = Tokenizer(text)
    vocab_size = tokenizer.vocab_size

    data = tokenizer.encode(text)
    print(f"Length of data: {len(data)}")

    split = int(len(data) * train_split)
    train = data[:split]
    test = data[split:]
    print(f"Train size: {len(train)}")
    print(f"Test size: {len(test)}")

    model = BigramModel(vocab_size)

    train_loop(1000, model, train)

    print("Generating text...")
    text = model.generate_text(torch.zeros((1, 1), dtype=torch.long))
    decoded = tokenizer.decode(text[0])

    print(decoded)
