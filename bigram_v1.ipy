import time

import torch

from transformer import MultiHeadAttention, Tokenizer

train_split = 0.8
block_size = 256
batch_size = 256
n_embed = 32
n_heads = 16
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def load_data() -> str:
    with open("data/shakespeare.txt", "r") as f:
        text = f.read()

    return text


def get_batch(data, block_size=block_size, batch_size=batch_size):
    offsets = torch.randint(0, split - block_size, (batch_size,))

    xb = torch.stack([data[offset : offset + block_size] for offset in offsets])
    yb = torch.stack([data[offset + 1 : offset + block_size + 1] for offset in offsets])

    return xb, yb


class BigramModel(torch.nn.Module):
    def __init__(self, vocab_size):
        super().__init__()

        self.vocab_size = vocab_size

        self.embedding = torch.nn.Embedding(vocab_size, n_embed)
        self.pos_embedding = torch.nn.Embedding(block_size, n_embed)

        self.sa_heads = MultiHeadAttention(
            n_heads=n_heads,
            head_size=n_embed // n_heads,
            token_size=n_embed,
            block_size=block_size,
        )

        self.lm_head = torch.nn.Linear(n_embed, vocab_size)

    def forward(self, x, targets=None):
        B, T = x.shape

        token_embeddings = self.embedding(x)
        pos_embeddings = self.pos_embedding(torch.arange(T).to(x.device))

        token_embeddings += pos_embeddings

        token_embeddings = self.sa_heads(token_embeddings)

        logits = self.lm_head(token_embeddings)

        if targets is None:
            return logits, None

        loss = torch.nn.functional.cross_entropy(
            logits.view(-1, self.vocab_size), targets.view(-1)
        )

        return logits, loss

    def generate_text(self, x, steps=500):
        for _ in range(steps):
            truncated_x = x[:, -block_size:]
            logits, _ = self(truncated_x)
            last_logits = logits[:, -1, :]  # B x T x C
            probs = torch.functional.F.softmax(last_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            x = torch.cat([x, next_token], dim=1)

        return x

    @torch.no_grad()
    def generate(self, x, steps=100):
        for _ in range(steps):
            x, _ = self(x)
            x = x[-1].argmax()
            yield x


def train_loop(
    n_batches: int, model: BigramModel, data: torch.Tensor, lr: float = 3e-3
):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    print_every = int(n_batches * 0.1)

    for epoch in range(n_batches):
        xb, yb = get_batch(data)

        optimizer.zero_grad()
        _, loss = model(xb, yb)

        if epoch % print_every == 0:
            print(f"Epoch {epoch}, loss: {loss.item()}")

        loss.backward()
        optimizer.step()


if __name__ == "__main__":
    text = load_data()
    tokenizer = Tokenizer(text)
    vocab_size = tokenizer.vocab_size

    data = tokenizer.encode(text).to(device)
    print(f"Length of data: {len(data)}")

    split = int(len(data) * train_split)
    train = data[:split]
    test = data[split:]
    print(f"Train size: {len(train)}")
    print(f"Test size: {len(test)}")

    model = BigramModel(vocab_size).to(device)

    print(f"Training model on device {device}...")
    start = time.time()
    train_loop(5000, model, train)
    duration = time.time() - start
    print(f"Training took {duration:.2f} seconds")

    print("Generating text...")
    text = model.generate_text(torch.zeros((1, 1), dtype=torch.long))
    decoded = tokenizer.decode(text[0])

    print(decoded)
